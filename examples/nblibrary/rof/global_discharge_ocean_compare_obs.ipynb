{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## NB2. ROF monthly, annual, seasonal discharge at ocean outlets <a id='top'></a>\n",
    "\n",
    "Use \n",
    "\n",
    "1. reach-D19 gauge link ascii\n",
    "2. D19 flow site geopackage\n",
    "3. D19 discharge netCDF\n",
    "4. monthly and yearly flow netCD (history file)\n",
    "\n",
    "[1. Setupt](#setup)\n",
    "\n",
    "[2. Loading data](#load_data)\n",
    "\n",
    "- monthly history files (directory from CESM or postprocessed) from archive. \n",
    "\n",
    "- Reference data is monthly discharge estimates at 922 big river mouths from Dai et al. 2019 data (D19)\n",
    "\n",
    "[3. Read river, catchment, gauge information](#read_meta)\n",
    "\n",
    "- catchment polygon (geopackage)\n",
    "- gauge point (geopackage)\n",
    "- gauge-catchment link (csv)\n",
    "- outlet reach information (netCDF) including discharging ocean names\n",
    "\n",
    "[4. Ocean discharge line plots](#922_rivers)\n",
    "\n",
    "- total seasonal flow for oceans. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import cartopy.feature as cfeature\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from scripts.utility import load_yaml\n",
    "from scripts.utility import no_time_variable\n",
    "from scripts.utility import read_shps\n",
    "from scripts.utility import get_index_array\n",
    "\n",
    "rivers_50m = cfeature.NaturalEarthFeature(\"physical\", \"rivers_lake_centerlines\", \"50m\")\n",
    "land = cfeature.LAND\n",
    "\n",
    "print(\"\\nThe Python version: %s.%s.%s\" % sys.version_info[:3])\n",
    "print(xr.__name__, xr.__version__)\n",
    "print(pd.__name__, pd.__version__)\n",
    "print(gpd.__name__, gpd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "## 1. Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameter Defaults\n",
    "# parameters are set in CUPiD's config.yml file\n",
    "\n",
    "CESM_output_dir = \"\"\n",
    "case_name = None  # case name\n",
    "base_case_name = None  # base case name\n",
    "start_date = \"\"\n",
    "end_date = \"\"\n",
    "serial = False  # use dask LocalCluster\n",
    "lc_kwargs = {}\n",
    "\n",
    "analysis_name = \"test\"  # Used for Figure png names\n",
    "rof_start_date = start_date  # specify if different starting yyyy-mm-dd is desired\n",
    "rof_end_date = end_date  # specify if different ending yyyy-mm-dd is desired\n",
    "grid_name = \"f09_f09_mosart\"  # ROF grid name used in case\n",
    "base_grid_name = grid_name  # spcify ROF grid name for base_case if different than case\n",
    "figureSave = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "ROF ancillary data specification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup = load_yaml(\"./setup/setup.yaml\")\n",
    "\n",
    "domain_dir = setup[\n",
    "    \"ancillary_dir\"\n",
    "]  # ancillary directory including such as ROF domain, river network data\n",
    "geospatial_dir = setup[\"geospatial_dir\"]  # including shapefiles or geopackages\n",
    "ref_flow_dir = setup[\"ref_flow_dir\"]  # including observed or reference flow data\n",
    "case_meta = setup[\"case_meta\"]  # RO grid meta\n",
    "catch_gpkg = setup[\"catch_gpkg\"]  # catchment geopackage meta\n",
    "reach_gpkg = setup[\"reach_gpkg\"]  # reach geopackage meta\n",
    "network_nc = setup[\"river_network\"]  # river network meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oceans_list = [\n",
    "    \"arctic\",\n",
    "    \"atlantic\",\n",
    "    \"indian\",\n",
    "    \"mediterranean\",\n",
    "    \"pacific\",\n",
    "    \"south_china\",\n",
    "    \"global\",\n",
    "]\n",
    "case_list = [case for case in [case_name, base_case_name] if case is not None]\n",
    "grid_list = [grid for grid in [grid_name, base_grid_name] if grid is not None]\n",
    "time_period = slice(f\"{rof_start_date}\", f\"{rof_end_date}\")  # analysis time period\n",
    "nyrs = int(rof_end_date[:4]) - int(rof_start_date[:4]) + 1  # number of years\n",
    "nmons = nyrs * 12  # number of months\n",
    "year_list = [\n",
    "    \"{:04d}\".format(yr)\n",
    "    for yr in np.arange(int(rof_start_date[0:4]), int(rof_end_date[0:4]) + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### dasks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spin up cluster (if running in parallel)\n",
    "client = None\n",
    "if serial:\n",
    "    cluster = LocalCluster(**lc_kwargs)\n",
    "    client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data <a id='load_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Monthly/annual flow netCDFs\n",
    "- month_data (xr dataset)\n",
    "- year_data (xr dataset)\n",
    "- seas_data (xr dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reachID = {}\n",
    "month_data = {}\n",
    "year_data = {}\n",
    "seas_data = {}\n",
    "for case, grid in zip(case_list, grid_list):\n",
    "    in_dire = os.path.join(CESM_output_dir, case, \"rof/hist\")\n",
    "    model = case_meta[grid][\"model\"]\n",
    "    domain = case_meta[grid][\"domain_nc\"]\n",
    "    var_list = case_meta[grid][\"vars_read\"]\n",
    "\n",
    "    def preprocess(ds):\n",
    "        return ds[var_list]\n",
    "\n",
    "    # monthly\n",
    "    nc_list = []\n",
    "    for nc_path in sorted(glob.glob(f\"{in_dire}/{case}.{model}.h*.????-*.nc\")):\n",
    "        for yr in year_list:\n",
    "            if yr in os.path.basename(nc_path):\n",
    "                nc_list.append(nc_path)\n",
    "    month_data[case] = (\n",
    "        xr.open_mfdataset(\n",
    "            nc_list,\n",
    "            data_vars=\"minimal\",\n",
    "            parallel=True,\n",
    "            preprocess=preprocess,\n",
    "        )\n",
    "        .sel(time=time_period)\n",
    "        .load()\n",
    "    )\n",
    "    # annual\n",
    "    year_data[case] = month_data[case].resample(time=\"YS\").mean(dim=\"time\")\n",
    "\n",
    "    # seasonal (compute here instead of reading for conisistent analysis period)\n",
    "    seas_data[case] = month_data[case].groupby(\"time.month\").mean(\"time\")\n",
    "    vars_no_time = no_time_variable(month_data[case])\n",
    "    if vars_no_time:\n",
    "        seas_data[case][vars_no_time] = seas_data[case][vars_no_time].isel(\n",
    "            month=0, drop=True\n",
    "        )\n",
    "    time = month_data[case][\"time\"]\n",
    "    if domain == \"None\":\n",
    "        reachID[case] = month_data[case][\"reachID\"].values\n",
    "    else:\n",
    "        reachID[case] = (\n",
    "            xr.open_dataset(f\"{domain_dir}/{domain}\")[\"reachID\"]\n",
    "            .stack(seg=(\"lat\", \"lon\"))\n",
    "            .values\n",
    "        )\n",
    "    print(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 D19 discharge data\n",
    "- ds_q_obs_mon (xr datasets)\n",
    "- ds_q_obs_yr (xr datasets)\n",
    "- dr_q_obs_seasonal (xr datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read monthly data\n",
    "ds_q = xr.open_dataset(\n",
    "    \"%s/D09/coastal-stns-Vol-monthly.updated-May2019.mod.nc\" % (ref_flow_dir),\n",
    "    decode_times=False,\n",
    ")\n",
    "ds_q[\"time\"] = xr.cftime_range(\n",
    "    start=\"1900-01-01\", end=\"2018-12-01\", freq=\"MS\", calendar=\"standard\"\n",
    ")\n",
    "\n",
    "# monthly\n",
    "obs_available = True\n",
    "if ds_q[\"time\"].sel(time=time_period).values.size == 0:\n",
    "    obs_available = False\n",
    "    ds_q_obs_mon = xr.DataArray(\n",
    "        data=np.ones((len(time), len(ds_q[\"station\"])), dtype=\"float\") * np.nan,\n",
    "        dims=[\"time\", \"station\"],\n",
    "        coords=dict(\n",
    "            station=ds_q[\"station\"],\n",
    "            time=time,\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    ds_q_obs_mon = ds_q[\"FLOW\"].sel(time=time_period)\n",
    "# compute annual flow from monthly\n",
    "ds_q_obs_yr = ds_q_obs_mon.resample(time=\"YE\").mean(dim=\"time\")\n",
    "# compute annual cycle at monthly scale\n",
    "dr_q_obs_seasonal = ds_q_obs_mon.groupby(\"time.month\").mean(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading river, catchment, gauge infomation  <a id='read_meta'></a>\n",
    "\n",
    "- catchment polygon (geopackage)\n",
    "- gauge point (geopackage)\n",
    "- gauge-catchment link (csv)\n",
    "- outlet reach information (netCDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. reach-D19 gauge link csv\n",
    "- gauge_reach_lnk (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_reach_lnk = {}\n",
    "for case, grid in zip(case_list, grid_list):\n",
    "    gauge_reach_lnk[case] = pd.read_csv(\n",
    "        \"%s/D09/D09_925.%s.asc\" % (ref_flow_dir, case_meta[grid][\"network\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 D19 flow site shapefile\n",
    "- gauge_shp (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gauge_shp = gpd.read_file(\n",
    "    os.path.join(ref_flow_dir, \"D09\", \"geospatial\", \"D09_925.gpkg\")\n",
    ")\n",
    "gauge_shp = gauge_shp[gauge_shp[\"id\"] != 9999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ocean_shp = gpd.read_file(os.path.join(geospatial_dir, \"oceans.gpkg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Read river network information\n",
    "- riv_ocean (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## read catchment geopackage\n",
    "gdf_cat = {}\n",
    "for case, grid in zip(case_list, grid_list):\n",
    "    cat_gpkg = os.path.join(\n",
    "        geospatial_dir, catch_gpkg[grid][\"file_name\"]\n",
    "    )  # geopackage name\n",
    "    id_name_cat = catch_gpkg[grid][\"id_name\"]  # reach ID in geopackage\n",
    "    var_list = [id_name_cat]\n",
    "    if \"lk\" in grid_name:\n",
    "        var_list.append(\"lake\")\n",
    "    gdf_cat[case] = read_shps([cat_gpkg], var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read river outlet netcdf\n",
    "riv_ocean = {}\n",
    "for case, grid in zip(case_list, grid_list):\n",
    "    riv_ocean_file = os.path.join(\n",
    "        domain_dir, network_nc[grid][\"file_name\"].replace(\".aug.nc\", \".outlet.nc\")\n",
    "    )  # network netcdf name\n",
    "    ds_rn_ocean = xr.open_dataset(riv_ocean_file).set_index(seg=\"seg_id\")\n",
    "    df_tmp = ds_rn_ocean.to_dataframe()\n",
    "    riv_ocean[case] = pd.merge(\n",
    "        gdf_cat[case], df_tmp, left_on=catch_gpkg[grid][\"id_name\"], right_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Merge gauge, outlet catchment dataframe\n",
    "\n",
    "- gauge_shp1 (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Merge gauge_reach lnk (dataframe) into gauge shapefile\n",
    "gauge_shp1 = {}\n",
    "for case, grid in zip(case_list, grid_list):\n",
    "    df = gauge_reach_lnk[case]\n",
    "\n",
    "    # df = df.loc[(df['flag'] == 0)]\n",
    "    df1 = df.drop(columns=[\"riv_name\"])\n",
    "    df2 = pd.merge(gauge_shp, df1, how=\"inner\", left_on=\"id\", right_on=\"gauge_id\")\n",
    "    gauge_shp1[case] = pd.merge(\n",
    "        df2,\n",
    "        riv_ocean[case],\n",
    "        how=\"inner\",\n",
    "        left_on=\"route_id\",\n",
    "        right_on=catch_gpkg[grid][\"id_name\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "------\n",
    "## 4. plot annual cycle for global oceans <a id='24_large_rivers'></a>\n",
    "\n",
    "TODO: Referece flow plot should be independent from cases (network). Currently the last case plotted looks better matched with reference flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "nrows = 4\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(7.25, 6.5))\n",
    "plt.subplots_adjust(\n",
    "    top=0.95, bottom=0.065, right=0.98, left=0.10, hspace=0.225, wspace=0.250\n",
    ")  # create some space below the plots by increasing the bottom-value\n",
    "\n",
    "for ix, ocean_name in enumerate(oceans_list):\n",
    "    row = ix // 2\n",
    "    col = ix % 2\n",
    "    for case, grid in zip(case_list, grid_list):\n",
    "\n",
    "        q_name = case_meta[grid][\"flow_name\"]\n",
    "\n",
    "        if case_meta[grid][\"network_type\"] == \"vector\":\n",
    "            if ocean_name == \"global\":\n",
    "                id_list = gauge_shp1[case][\"route_id\"].values\n",
    "            else:\n",
    "                id_list = gauge_shp1[case][gauge_shp1[case][\"ocean\"] == ocean_name][\n",
    "                    \"route_id\"\n",
    "                ].values\n",
    "            reach_index = get_index_array(reachID[case], id_list)\n",
    "            dr_flow = seas_data[case][q_name].isel(seg=reach_index).sum(dim=\"seg\")\n",
    "            dr_flow.plot(ax=axes[row, col], linestyle=\"-\", lw=0.75, label=case)\n",
    "\n",
    "        elif case_meta[grid_name][\"network_type\"] == \"grid\":  # means 2d grid\n",
    "            if ocean_name == \"global\":\n",
    "                id_list = gauge_shp1[case][\"route_id\"].values\n",
    "            else:\n",
    "                id_list = gauge_shp1[case][gauge_shp1[case][\"ocean\"] == ocean_name][\n",
    "                    \"route_id\"\n",
    "                ].values\n",
    "\n",
    "            reach_index = get_index_array(reachID[case], id_list)\n",
    "            seas_data_vector = seas_data[case][q_name].stack(seg=(\"lat\", \"lon\"))\n",
    "            dr_flow = seas_data_vector.isel(seg=reach_index).sum(dim=\"seg\")\n",
    "            dr_flow.plot(ax=axes[row, col], linestyle=\"-\", lw=0.75, label=case)\n",
    "\n",
    "    # reference data\n",
    "    if obs_available:\n",
    "        if ocean_name == \"global\":\n",
    "            id_list = gauge_shp1[case][\"id\"].values\n",
    "        else:\n",
    "            id_list = gauge_shp1[case][gauge_shp1[case][\"ocean\"] == ocean_name][\n",
    "                \"id\"\n",
    "            ].values\n",
    "        gauge_index = get_index_array(ds_q[\"id\"].values, id_list)\n",
    "        dr_obs = dr_q_obs_seasonal.isel(station=gauge_index).sum(dim=\"station\")\n",
    "        dr_obs.plot(\n",
    "            ax=axes[row, col],\n",
    "            linestyle=\"None\",\n",
    "            marker=\"o\",\n",
    "            markersize=2,\n",
    "            c=\"k\",\n",
    "            label=\"D19\",\n",
    "        )\n",
    "\n",
    "    axes[row, col].set_title(\"%d %s\" % (ix + 1, ocean_name), fontsize=9)\n",
    "    axes[row, col].set_xlabel(\"\")\n",
    "    if row < 7:\n",
    "        axes[row, col].set_xticklabels(\"\")\n",
    "    if col == 0:\n",
    "        axes[row, col].set_ylabel(\"Mon. flow [m$^3$/s]\", fontsize=9)\n",
    "    else:\n",
    "        axes[row, col].set_ylabel(\"\")\n",
    "    axes[row, col].tick_params(\"both\", labelsize=\"x-small\")\n",
    "\n",
    "# Legend- make space below the plot-raise bottom. there will be an label below the second last (bottom middle) ax, thanks to the bbox_to_anchor=(x, y) with a negative y-value.\n",
    "axes[row, col].legend(\n",
    "    loc=\"center left\", bbox_to_anchor=(1.10, 0.40, 0.75, 0.1), ncol=1, fontsize=\"small\"\n",
    ")\n",
    "\n",
    "for jx in range(ix + 1, nrows * ncols):\n",
    "    row = jx // 2\n",
    "    col = jx % 2\n",
    "    fig.delaxes(axes[row][col])\n",
    "\n",
    "if figureSave:\n",
    "    plt.savefig(f\"./NB2_Fig1_ocean_discharge_season_{analysis_name}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupid-analysis",
   "language": "python",
   "name": "cupid-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
